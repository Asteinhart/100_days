---
title: "articles"
author: "Austin Steinhart"
date: "5/21/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)

articles_raw <- read_csv('/Volumes/GoogleDrive/My Drive/data_stuff/github/100_days_gh/100_days_data/articles.csv')

topics <- sort(unique(articles_raw$`Primary Topic`))

articles <- articles_raw %>%
  transmute(
    article_title = `Name`,
    publication = factor(`Publication`, levels = unique(articles_raw$`Publication`)),
    rating = `Rating`,
    topic = factor(`Primary Topic`, levels = topics),
    tags = `Tags`,
    date_published = mdy(`Date`),
    date_added = mdy(`Dated Added`),
    link = `Link`,
    summary = `Summary`,
    top5 = `Top 5`,
  )

articles$publication = addNA(articles$publication)

summary(articles)
```

1. Context to artilces and some back story with the notes app. why I like articles. switch to airtable. rational to design of table? (maybe further down) 
screen shot of airtable?

2. Overview (total articles, number of different publications,top 5 topics?)

3. Articles by Topic
- topics added over time (top topics by year)
- bubble graphic of topics
- title world cloud

4. Articles over time 
 per year
- top months

5. Aticles by Publication
 over tops publications, (publicatiosn by year)

6. Favorite articles highlight

7. interactive table to search

```{r}
#OVERVIEW
#total numbers of articles 217
nrow(articles)

# added 112 in 2018, 51 in 2019, 33 in 2020, and 21 in 2021 so far
# put thema all in the spread sheet in 2018 so nto sure on states for 2016, 2017 or 2018
articles %>%
  group_by(year(date_added)) %>%
  tally()

# not added after 2018 include 50 that were published in 2018, 38 that were published in 2017 and 23 published before 2017
articles %>%
  filter(year(date_added) < 2019) %>%
  group_by(year(date)) %>% 
  tally()
```


```{r}
#TOPICS
# 15 unique topics
sort(unique(articles$topic))
length(unique(articles$topic))

# Life (40), Metro (37), Politics (29), Higher Education (23), Education (17), Stockton  (17), Education ties for first (40) if combine
articles %>%
  group_by(topic) %>%
  count() %>%
  arrange(desc(n))

topic_count <- articles %>%
  group_by(year = year(date_added), topic) %>%
  count()

write.csv(topic_count,'topic_count.csv')

# top topic by year with count
articles %>%
  group_by(year = year(date_added), topic) %>%
  count() %>%
  ungroup() %>%
  group_by(year)%>%
  filter(n == max(n))
```

```{r}
#title word cloud
#install.packages("tm")
library(tm)

#install.packages("wordcloud")
library(wordcloud)

text <- articles$article_title
# Create a corpus  
docs <- Corpus(VectorSource(text))

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))

dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)

wordcloud(words = df$word, freq = df$freq, min.freq = 3, max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))

```



```{r}
# PUBLICATIONS
# 46 unique publications, actually 42 but...
length((unique(articles$publication)))

articles %>%
  group_by(publication) %>%
  count() %>%
  arrange(desc(n))

# group same publications togehter
articles_pub = articles %>%
  mutate(publication = fct_collapse(publication,
   'The New York Times' = c("The New York Times", "The New York Times,The Upshot", "The Upshot,The New York Times", "The New York Times,The New York Times Magazine"),
    'The Economist' = c("The Economist", "The Economist,1843 Magazine")
  ))


# 5 publications (5/42) account for 152/217 (70%) of articles.
# NYT, Atlantic, 538, New Yorker, Vox
articles_pub %>%
  group_by(publication) %>%
  count() %>%
  arrange(desc(n))



```


```{r}
# find all publciations that only have one entry and change publication to other
articles_one = articles %>%
  group_by(publication)%>%
  tally()%>%
  filter(n == 1)
one_pub = unlist(str_split(toString(articles_one$publication), ", "))
one_pub = append(one_pub, NA)

# hannah

articles_pub = articles %>%
  mutate(publication = fct_collapse(publication,
   'The New York Times' = c("The New York Times", "The New York Times,The Upshot", "The Upshot,The New York Times", "The New York Times,The New York Times Magazine"),
    'The Economist' = c("The Economist", "The Economist,1843 Magazine")
  ))%>%
  
  mutate(publication = fct_collapse(publication,
   Other = one_pub 
  ))

#graph of top publications
articles_pub %>%
  group_by(publication) %>%
  tally()%>%
  arrange(desc(n))%>%
  ggplot(aes(reorder(publication, -n),n))+
  geom_bar(stat = 'identity')+
  scale_y_continuous(name = "Amount ", breaks=seq(0, 80, 5))+
  coord_flip()

# articles by publication by topic
articles_pub %>%
  group_by(publication) %>%
  ggplot(aes(publication, )) +
  geom_bar(aes(color = topic, position = "identity")) +
  scale_y_continuous(name = "Amount ", breaks=seq(0, 80, 5)) +
  coord_flip()

# articles by topic
articles_pub %>%
 # group_by(topic) %>%
  ggplot(aes(forcats::fct_infreq(topic))) +
  geom_bar() +
  coord_flip()
nrow(articles_pub)

# TODO accumilation of articles added (like the good reads list)
articles_c = articles_pub %>%
  arrange(date_added) %>%
  mutate(total_read = 1:nrow(articles_pub))

articles_c %>%
  filter(date_added != as.Date("2018-12-15")) %>%
ggplot(aes(date_added, total_read))+
  geom_point()+
  geom_line()

# TODO growth of topic added by data so see reading habit and interest change


```
